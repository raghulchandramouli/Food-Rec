{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "09J2WyAPe8G6",
        "outputId": "9e02900b-dad7-4903-ca50-263d0403abc9"
      },
      "outputs": [],
      "source": [
        "!pip install langchain google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_AsJ1mjfSav"
      },
      "outputs": [],
      "source": [
        "### 1.  calling Library:\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from langchain_core.runnables.base import RunnableSequence ### for serialization\n",
        "from langchain.prompts import PromptTemplate\n",
        "from typing import Optional, List\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt8SqzSthAbW"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "import google.generativeai as genai\n",
        "\n",
        "class GeminiLLM:\n",
        "    def __init__(self, model_name: str, api_key: str, generation_config: dict):\n",
        "        \"\"\"\n",
        "        Initializes the GeminiLLM object with the provided model name, API key, and generation configuration.\n",
        "        \n",
        "        Args:\n",
        "        - model_name: The name of the model to be used (e.g., 'gemini-2.0').\n",
        "        - api_key: The API key used to authenticate with the Generative AI service.\n",
        "        - generation_config: A dictionary containing configuration options for text generation.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name  # Store the model name\n",
        "        self.api_key = api_key  # Store the API key\n",
        "        self.generation_config = generation_config  # Store the generation configuration\n",
        "        genai.configure(api_key=api_key)  # Configure the genai API with the provided API key\n",
        "\n",
        "    def __call__(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generates a response from the model based on the provided prompt.\n",
        "        \n",
        "        Args:\n",
        "        - prompt: The text input that serves as the prompt for text generation.\n",
        "        - stop: An optional list of stop sequences. If a stop sequence is found in the output, \n",
        "                the response will be truncated at that point.\n",
        "\n",
        "        Returns:\n",
        "        - A string containing the generated text.\n",
        "        \"\"\"\n",
        "        # Call the text generation API with the provided parameters\n",
        "        response = genai.generate_text(\n",
        "            model=self.model_name,  # Specify the model to use\n",
        "            prompt=prompt,  # Provide the prompt\n",
        "            temperature=self.generation_config.get(\"temperature\", 1),  # Control randomness in output\n",
        "            top_p=self.generation_config.get(\"top_p\", 0.95),  # Top-p sampling\n",
        "            top_k=self.generation_config.get(\"top_k\", 40),  # Top-k sampling\n",
        "            max_output_tokens=self.generation_config.get(\"max_output_tokens\", 8192),  # Limit output tokens\n",
        "            response_mime_type=self.generation_config.get(\"response_mime_type\", \"text/plain\"),  # Response format\n",
        "        )\n",
        "        \n",
        "        # Check if the response has a 'text' attribute or 'generated_text', and assign it accordingly\n",
        "        text = response.text if hasattr(response, 'text') else response.generated_text\n",
        "\n",
        "        # If stop sequences are provided, truncate the output at the first stop sequence\n",
        "        if stop:\n",
        "            for stop_seq in stop:\n",
        "                if stop_seq in text:\n",
        "                    text = text.split(stop_seq)[0]  # Truncate at the first occurrence of a stop sequence\n",
        "\n",
        "        return text  # Return the generated (and possibly truncated) text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v7UMuhPjCWW"
      },
      "outputs": [],
      "source": [
        "### 3. Load Recipes\n",
        "\n",
        "def load_recipes(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        recipes = json.load(file)\n",
        "    return recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoDyiMx7jPbU"
      },
      "outputs": [],
      "source": [
        "def search_recipes(query: str, recipes: list, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Searches for recipes in a list based on a query. The function will return the top_k recipes \n",
        "    that match the query in their name, ingredients, or cuisine.\n",
        "\n",
        "    Args:\n",
        "    - query: The search query string (e.g., an ingredient, dish name, or cuisine).\n",
        "    - recipes: A list of recipes where each recipe is represented as a dictionary with keys like 'name', 'ingredients', and 'cuisine'.\n",
        "    - top_k: The maximum number of matching recipes to return (default is 5).\n",
        "\n",
        "    Returns:\n",
        "    - A list of recipes that match the search query.\n",
        "    \"\"\"\n",
        "    query = query.lower()  # Convert the query to lowercase to make the search case-insensitive\n",
        "    results = []  # Initialize an empty list to store matching recipes\n",
        "\n",
        "    # Iterate over each recipe in the list of recipes\n",
        "    for recipe in recipes:\n",
        "        name = recipe['name'].lower()  # Convert the recipe name to lowercase\n",
        "        ingredients = ' '.join(recipe['ingredients']).lower()  # Join ingredients and convert to lowercase\n",
        "        cuisine = recipe['cuisine'].lower()  # Convert the cuisine to lowercase\n",
        "\n",
        "        # Check if the query matches any part of the recipe (name, ingredients, or cuisine)\n",
        "        if (query in name) or (query in ingredients) or (query in cuisine):\n",
        "            results.append(recipe)  # If match is found, add the recipe to results\n",
        "\n",
        "        # Stop early if we have already found top_k matches\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "\n",
        "    return results  # Return the list of matching recipes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FkoKDkNkjAM"
      },
      "outputs": [],
      "source": [
        "def setup_pipeline(model_name: str, api_key: str, generation_config: dict):\n",
        "    \"\"\"\n",
        "    Sets up a Retrieval-Augmented Generation (RAG) pipeline with a given model and configuration.\n",
        "    This pipeline allows for retrieving relevant recipes based on a user query and generating \n",
        "    a detailed response using a language model.\n",
        "\n",
        "    Args:\n",
        "    - model_name: The name of the AI model (e.g., 'gemini-2.0').\n",
        "    - api_key: The API key to authenticate the request to the Generative AI service.\n",
        "    - generation_config: A dictionary of configuration options for the language model (e.g., temperature, top_k, etc.).\n",
        "\n",
        "    Returns:\n",
        "    - A callable pipeline function that takes a query and a list of recipes, then generates a detailed response.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the GeminiLLM class with the model name, API key, and generation configuration\n",
        "    gemini_llm = GeminiLLM(\n",
        "        model_name=model_name,  # Specify the model to use (e.g., gemini-2.0)\n",
        "        api_key=api_key,  # API key for authentication\n",
        "        generation_config=generation_config  # Configuration options for text generation\n",
        "    )\n",
        "\n",
        "    # Define the prompt template to structure the user query and retrieved recipes for the LLM\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"recipes\", \"query\"],  # Define the placeholders to be used in the template\n",
        "        template=\"\"\"\n",
        "            You are a culinary expert. Based on the following recipes, answer the user's query in detail.\n",
        "\n",
        "            ### Retrieved Recipes:\n",
        "            {recipes}\n",
        "\n",
        "            ### User Query:\n",
        "            {query}\n",
        "\n",
        "            ### Response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Wrap the gemini_llm in a lambda function so it can be invoked in the pipeline sequence\n",
        "    # The lambda takes the input dictionary (which will contain the 'query') and sends it to the gemini_llm for generation\n",
        "    pipeline = RunnableSequence([prompt_template, lambda x: gemini_llm(prompt=x['query'])])\n",
        "\n",
        "    # Return the constructed pipeline function that combines the prompt template and model response generation\n",
        "    return pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7tPCv2hlN4t"
      },
      "outputs": [],
      "source": [
        "def get_recipe_suggestion(user_query: str, recipes: list, pipeline: RunnableSequence):\n",
        "    \"\"\"\n",
        "    This function takes a user query, searches for matching recipes, and generates a recipe suggestion\n",
        "    using a predefined RAG pipeline. If no matching recipes are found, it provides a general suggestion.\n",
        "\n",
        "    Args:\n",
        "    - user_query: The user's query or request for recipe suggestions.\n",
        "    - recipes: A list of available recipes (each recipe is a dictionary containing 'name', 'ingredients', and 'cuisine').\n",
        "    - pipeline: The RAG pipeline that generates a response using the recipes and query.\n",
        "\n",
        "    Returns:\n",
        "    - A response generated by the pipeline based on the query and matching recipes.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Search for matching recipes based on the user query (top 5 results)\n",
        "    matching_recipes = search_recipes(user_query, recipes, top_k=5)\n",
        "\n",
        "    if not matching_recipes:\n",
        "        # If no recipes are found, create a fallback message indicating no specific recipes were found\n",
        "        recipes_text = \"No specific recipes were found, but please provide a general suggestion based on the query.\"\n",
        "    else:\n",
        "        # If matching recipes are found, format them into a text string\n",
        "        recipes_text = \"\\n\\n\".join(\n",
        "            # For each matching recipe, format the name, ingredients, and cuisine\n",
        "            f\"**Name**: {recipe['name']}\\n**Ingredients**: {', '.join(recipe['ingredients'])}\\n**Cuisine**: {recipe['cuisine']}\"\n",
        "            for recipe in matching_recipes\n",
        "        )\n",
        "\n",
        "    # Run the pipeline to generate a response, passing the recipes (or fallback message) and the user query\n",
        "    response = pipeline.invoke({\"recipes\": recipes_text, \"query\": user_query})\n",
        "    \n",
        "    # Return the generated response from the pipeline\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTGvrT_glm4m"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that sets up the pipeline, loads the recipe data, and processes a user query\n",
        "    to generate a recipe suggestion using the Gemini language model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the model name that will be used for text generation\n",
        "    model_name = \"gemini-2.0-flash-exp\"\n",
        "    \n",
        "    # API key for authenticating the request to the AI model (replace with actual API key)\n",
        "    api_key = \"\"  # Replace with your actual API key\n",
        "    \n",
        "    # Define the configuration settings for the language model's response generation\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.7,  # Controls randomness in the output (0.7 is balanced)\n",
        "        \"top_p\": 0.9,        # Controls the diversity of the output (higher value means less diversity)\n",
        "        \"top_k\": 50,         # Controls the number of possible next tokens to sample from (higher value increases options)\n",
        "        \"max_output_tokens\": 1500,  # Sets the maximum number of tokens in the output (response length)\n",
        "        \"response_mime_type\": \"text/plain\",  # Specifies the format of the response (plain text)\n",
        "    }\n",
        "\n",
        "    # Load the recipe data from a file ('recipes.json' is assumed to be the file containing the recipes)\n",
        "    recipes = load_recipes(\"recipes.json\")\n",
        "    \n",
        "    # Set up the pipeline by passing the model name, API key, and generation configuration\n",
        "    pipeline = setup_pipeline(model_name, api_key, generation_config)\n",
        "\n",
        "    # Define the user query for the recipe suggestion\n",
        "    user_query = \"Suggest a recipe using chicken and garlic that fits Italian cuisine.\"\n",
        "\n",
        "    # Get the recipe suggestion by passing the user query, recipe data, and the pipeline\n",
        "    response = get_recipe_suggestion(user_query, recipes, pipeline)\n",
        "    \n",
        "    # Print the response from the Gemini model, which is the recipe suggestion\n",
        "    print(\"Gemini's Recipe Suggestion:\\n\")\n",
        "    print(response)\n",
        "\n",
        "# Ensure the main function is executed when the script is run directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU8pzaK6EL2a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
